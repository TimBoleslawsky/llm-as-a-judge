\section{Evaluating the Evaluators}

\subsection{Why do we Evaluate the Evaluator}
Unlike classical automatic metrics that are taken as fixed yardsticks, LLM-based evaluators themselves must be assessed for reliability, agreement with humans, robustness, fairness, and calibration~\cite{li2024_llmsasjudges}. Large-scale studies show both strong potential and important limitations in judge behavior across tasks and settings~\cite{bavaresco2024judgebench, zheng2023judgelm}. We call this evaluation of the evaluator \emph{meta-evaluation}.

\subsection{Meta-Evaluation Metrics}
The most direct indicator, if an evaluator performs correctly or not, is agreement with human annotations.~\cite{fu2023gptscore} report that GPT-based evaluators often correlate more strongly with human preferences than classical metrics. Agreement is typically summarized via rank correlation (Spearman, Kendall) or pairwise accuracy~\cite{li2024_llmsasjudges, bavaresco2024judgebench}.

Given human (or adjudicated) annotations providing pairwise or scalar labels, we assess judges along several axes~\cite{li2024_llmsasjudges, bavaresco2024judgebench, hada2024metal, liang2022helm}:
\begin{itemize}
\item \textbf{Rank Correlation:} Measures how well an evaluator’s ranking of systems or models matches human rankings at the system/model level. Conceptually, human labels arise from a latent aggregation pipeline: (Level 0) intrinsic response attributes (accuracy, factuality, relevance, coherence, style/tone, safety, completeness, output interpretability, etc.); (Level 1) an implicit human utility $q^*(x,y) = \sum_k w_k f_k(x,y)$ combining those attribute scorers with task/population-dependent weights; (Level 2) the judge $J(x,y)$ attempting to approximate $q^*$ via instructions, rubric, and internal knowledge~\cite{li2024_llmsasjudges, liang2022helm}. Spearman or Kendall correlations summarize how well the ordering induced by $J$ recovers the ordering induced by $q^*$ as reflected in human annotations~\cite{bavaresco2024judgebench, zheng2023judgelm}. High correlation therefore indicates the judge captures the salient weighted mixture of underlying attributes, but it does not by itself reveal which dimensions are well or poorly modeled~\cite{stiennon2020learning, ouyang2022training}.
\item \textbf{Pairwise Agreement:} The proportion of pairwise comparisons where the judge chooses the same winner as human annotators. This is natural for preference-learning and comparison-based evaluation, widely used in RLHF and pairwise preference studies; it avoids calibration issues in raw scalar scores. Agreement can be aggregated across prompts or normalized by difficulty~\cite{liu2024pairs, bavaresco2024judgebench}.
\item \textbf{Fairness / Bias:} Equity of judgments across demographic, linguistic, topical, or provider subgroups. Analyses compare mean scores or decision rates between slices and test for statistically significant disparities, following dataset-slicing methodologies. Reducing subgroup gaps and auditing harms are key for reliable deployment~\cite{liang2022helm, hada2024metal}.
\end{itemize}
Although central to meta-evaluation, not all meta-evaluation metrics depend on human labels. Additional axes, that don't depend on them, include:
\begin{itemize}
\item \textbf{Calibration:} Intuitively: when the judge sounds 70\% sure that answer A is better than B, it should only be wrong about 30\% of the time. Calibration checks the match between predicted preference strengths (explicit scores or transformed score gaps) and empirical win frequencies using reliability diagrams and summary metrics like Expected Calibration Error (ECE) or Brier score~\cite{guo2017calibration, brier1950verification}. Even if a judge ranks systems well, poor calibration limits safe thresholding (auto-accept high-confidence passes), routing (escalate only uncertain cases), and ensemble weighting. RLHF preference models supply the raw scores to which calibration can be applied~\cite{stiennon2020learning, ouyang2022training}, while post-hoc methods can further adjust probabilities~\cite{niculescu2005predicting}.
\item \textbf{Robustness:} Stability to label-irrelevant perturbations such as swapping answer order, paraphrasing prompts, or harmless formatting changes. Robust judges show small changes in scores or decisions under these edits. Robustness is the answer to the question: “Does the judgment change when it shouldn’t (because meaning didn’t)?”. Empirical studies report notable order effects and prompt sensitivity in current LLM judges~\cite{li2024_llmsasjudges, hada2024metal, zheng2023judgelm}.
\item \textbf{Consistency:} can seem similar to robustness but is worth distinguishing. It answers the question: “Does the same judge, re-run on the exact same (x, candidate (s)) pair, yield the same outcome?”. Lower intra-judge variance indicates more dependable decisions; self-consistency or judge ensembling can improve stability. Consistency matters when aggregating multiple rubric items or using judges within larger pipelines~\cite{li2024_llmsasjudges}.
\end{itemize}

For code, execution-based evaluation we can also leverage functional correctness: HumanEval defines correctness as passing predefined tests~\citep{chen2021evaluating}. While~\cite{li2024_llmsasjudges} correctly point out that execution-only assessment misses requirement adherence and style, we can still use functional correctness as a ground truth to evaluate the evaluator.

\subsection{Open Challenges}
Persistent issues with meta-evaluation of evaluators include dependence on high-quality labeled anchors; domain transfer gaps (e.g., from dialogue to code); transparency and bias analysis; and multi-level evaluation that integrates objective tests with instruction adherence~\cite{li2024_llmsasjudges, chen2021evaluating}. Additionally, concerning are human disagreements and resulting noisy ``Gold'' Labels. Human preference labels are not monolithic: annotator disagreement can be substantial, especially on open-ended or stylistically nuanced tasks, producing a \\emph{distribution} of plausible preferences rather than a single deterministic ordering~\cite{stiennon2020learning, ouyang2022training}. Large-scale studies of LLM judges report that part of the apparent judge ``error'' is attributable to intrinsic rater variability and ambiguous prompts~\cite{bavaresco2024judgebench, li2024_llmsasjudges}.