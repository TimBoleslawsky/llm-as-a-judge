\section{Evaluating the Evaluators}

\subsection{Why do we Evaluate the Evaluator}
Unlike classical automatic metrics that are taken as fixed yardsticks, LLM-based evaluators themselves must be assessed for reliability, agreement with humans, robustness, fairness, and calibration~\cite{li2024_llmsasjudges}. Large-scale studies show both strong potential and important limitations in judge behavior across tasks and settings~\cite{bavaresco2024judgebench, zheng2023judgelm}. We call this evaluation of the evaluator \emph{meta-evaluation}.

\subsection{Meta-Evaluation Metrics}
The most direct indicator, if an evaluator performs correctly or not, is agreement with human annotations.~\cite{fu2023gptscore} report that GPT-based evaluators often correlate more strongly with human preferences than classical metrics. Agreement is typically summarized via rank correlation (Spearman, Kendall) or pairwise accuracy~\cite{li2024_llmsasjudges, bavaresco2024judgebench}.

Given human (or adjudicated) annotations providing pairwise or scalar labels, we assess judges along several axes~\cite{li2024_llmsasjudges, bavaresco2024judgebench, hada2024metal, liang2022helm}:
\begin{enumerate}
\item \textbf{Rank Correlation:} Measures how well an evaluator’s ranking of systems or models matches human rankings at the system/model level. Spearman or Kendall correlations are standard and provide a scale-invariant summary of agreement in summarization, MT, and dialogue. High correlation indicates the judge preserves relative ordering even if absolute scores differ~\cite{bavaresco2024judgebench, zheng2023judgelm}.
\item \textbf{Pairwise Agreement:} The proportion of pairwise comparisons where the judge chooses the same winner as human annotators. This is natural for preference-learning and comparison-based evaluation, widely used in RLHF and pairwise preference studies; it avoids calibration issues in raw scalar scores. Agreement can be aggregated across prompts or normalized by difficulty~\cite{liu2024pairs, bavaresco2024judgebench}.
\item \textbf{Calibration:} Assesses whether the judge’s stated confidence or implied preference probabilities reflect true frequencies. Expected calibration error and reliability diagrams check that, for example, a “70\% confident” preference is correct about 70\% of the time. Poor calibration undermines thresholding and auto-grading even when rank agreement is high~\cite{stiennon2020learning, ouyang2022training}.
\item \textbf{Robustness:} Stability to label-irrelevant perturbations such as swapping answer order, paraphrasing prompts, or harmless formatting changes. Robust judges show small changes in scores or decisions under these edits; large sensitivity signals prompt anchoring or order biases. Empirical studies report notable order effects and prompt sensitivity in current LLM judges~\cite{li2024_llmsasjudges, hada2024metal, zheng2023judgelm}.
\item \textbf{Fairness / Bias:} Equity of judgments across demographic, linguistic, topical, or provider subgroups. Analyses compare mean scores or decision rates between slices and test for statistically significant disparities, following dataset-slicing methodologies. Reducing subgroup gaps and auditing harms are key for reliable deployment~\cite{liang2022helm, hada2024metal}.
\item \textbf{Consistency:} Repeatability of judgments under re-prompts, different decoding seeds, or alternate chain-of-thoughts. Lower intra-judge variance indicates more dependable decisions; self-consistency or judge ensembling can improve stability. Consistency matters when aggregating multiple rubric items or using judges within larger pipelines~\cite{li2024_llmsasjudges}.
\end{enumerate}
Stopping criteria for an ``adequate'' judge may include thresholds such as $\rho \ge 0.85$, variance below $\epsilon$, expected calibration error (ECE) below a domain-specific bound, and bounded subgroup disparity~\cite{liang2022helm, bavaresco2024judgebench}. % TBD: Look through these and see if they make sense

For code, execution-based evaluation we can also leverage functional correctness: HumanEval defines correctness as passing predefined tests~\citep{chen2021evaluating}. However, execution-only misses requirement adherence and style; combining objective signals with judge scores balances efficiency and coverage~\cite{li2024_llmsasjudges}.

\subsection{Open Challenges (Condensed)}
Persistent issues include dependence on high-quality labeled anchors; domain transfer gaps (e.g., from dialogue to code); transparency and bias analysis; and multi-level evaluation that integrates objective tests with instruction adherence~\cite{li2024_llmsasjudges, chen2021evaluating}. Additional concerns: multilingual consistency~\cite{hada2024metal}, sensitivity to order/formatting~\cite{zheng2023judgelm}, calibration under distribution shift~\cite{bavaresco2024judgebench}, and robust integration of objective and subjective signals~\cite{liang2022helm}. Promising directions include adaptive judge ensembles, adversarial robustness audits, and uncertainty-aware escalation policies. %TBD