\section{Evaluation Metrics}
Metrics for evaluating LLMs as judges fall into three categories:

\subsection{Agreement with Human Judgment}
The most direct measure of an evaluator’s quality is agreement with human annotators.\citet{fu2023gptscore} introduced GPTScore, which correlates LLM-based evaluation with human preference judgments across text and code tasks. Similarly, \citet{xu2023survey} stress that correlation coefficients such as Kendall’s tau and Spearman’s rho are widely adopted to measure alignment.

\subsection{Functional Correctness}
For code generation, execution-based evaluation remains critical. HumanEval \citep{chen2021evaluating} defines correctness as passing predefined test cases. However, execution-only approaches are limited because they miss requirement-specific constraints not encoded in tests. Thus, judge models are increasingly combined with functional correctness tests to balance efficiency and comprehensiveness.

\subsection{Robustness and Bias Metrics}
Recent works emphasize robustness of LLM judges.\citet{wang2023aligning} propose adversarial settings where LLM judges must handle misleading or ambiguous instructions. They also explore judge consistency across paraphrased prompts and semantically equivalent but syntactically different outputs. Bias metrics, such as preference for verbosity or syntactic patterns, are being proposed to better characterize weaknesses of evaluators.

\section{Evaluating the Evaluator}
A particularly novel contribution of this field is the explicit \textit{evaluation of evaluators}. Unlike classical metrics, which assume the metric itself is ground truth, here researchers directly assess judge reliability.

\citet{fu2023gptscore} demonstrate that GPT-4-based judges often outperform traditional automatic metrics in correlation with human ratings, but they also show systematic biases (e.g., preferring longer outputs). Similarly, \citet{wang2023aligning} highlight that different LLM judges can disagree substantially, motivating ensemble or consensus-based evaluation frameworks.

Another perspective is \textit{calibration}. A well-calibrated judge should not only predict correctness but also assign confidence scores that reflect uncertainty. Early works suggest LLM judges often overstate confidence \citep{zheng2023judgelm}.


\subsection{Meta-Evaluation Metrics}
Given human (or adjudicated) annotations $H$ providing pairwise or scalar labels, we assess $J$ along multiple axes \cite{li2024_llmsasjudges, bavaresco2024judgebench, hada2024metal, liang2022helm}:
\begin{enumerate}
	\item \textbf{Rank Correlation:} $\rho = \operatorname{Spearman}( r_{J}, r_{H})$ over system-level or model-level aggregates \cite{bavaresco2024judgebench, zheng2023judgelm}.
	\item \textbf{Pairwise Agreement:} $A = \Pr[ \operatorname{sign}(J(x,y_i)-J(x,y_j)) = \operatorname{sign}(q_H(x,y_i)-q_H(x,y_j)) ]$ \cite{liu2024pairs, bavaresco2024judgebench}.
	\item \textbf{Calibration:} Reliability alignment between predicted preference probabilities $P(y_i \succ y_j)$ and empirical win rates \cite{stiennon2020learning, ouyang2022training}.
		\item \textbf{Robustness:} Sensitivity to answer order permutations ($\Delta_{\text{perm}}$) and prompt paraphrases ($\Delta_{\text{prompt}}$) \cite{li2024_llmsasjudges, hada2024metal, zheng2023judgelm}.
	\item \textbf{Fairness / Bias:} Disparity across subgroups $\mathcal{S}$: $\max_{s,s' \in \mathcal{S}} | \mathbb{E}[J \mid s] - \mathbb{E}[J \mid s'] |$ \cite{liang2022helm, hada2024metal}.
	\item \textbf{Consistency:} Intra-judge variance $\operatorname{Var}_m[ J(x,y) ]$ over $m$ stochastic passes (self-consistency / chain-of-thought re-asks) \cite{li2024_llmsasjudges}.
\end{enumerate}
Stopping criteria for an ``adequate'' judge may include thresholds such as $\rho \ge 0.85$, variance below $\epsilon$, expected calibration error (ECE) below a domain-specific bound, and bounded subgroup disparity \cite{liang2022helm, bavaresco2024judgebench}.

Judge models can complement binary signals by assessing stylistic clarity, efficiency, or safety not captured in tests~\cite{li2024_llmsasjudges}.