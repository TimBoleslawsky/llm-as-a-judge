
\section{Limitations and Challenges of LLM-as-Judge Evaluators}
This section highlights two foundational, cross-cutting open challenges that underlie many surface-level issues (robustness, fairness, calibration, transfer) discussed elsewhere.

\subsection{Human Annotator Disagreement and Uncertain Preferences}
Rather than a single latent ordering, many tasks yield a \emph{distribution} over plausible preferences for response pairs or rankings. Pairwise signals are frequently modeled via probabilistic frameworks (e.g., Bradley--Terry, Elo) acknowledging stochastic choice~\cite{bradley1952rank, elo1978rating, liu2024pairs}. However, meta-evaluation pipelines often collapse multiple votes into a single label without carrying forward uncertainty (vote entropy, variance). This conflation distorts rank correlation interpretation, inflates apparent calibration error on ambiguous pairs, and can bias fairness audits if disagreement clusters in specific subgroups or languages~\cite{hada2024metal, liang2022helm}.

Although posing a significant challenge, there are some promising mitigation directions. Among others, these include:
\begin{itemize}
	\item \textbf{Active / Targeted Adjudication:} Escalate only high-disagreement or high-impact pairs to expert adjudicators to refine a trusted ``gold'' subset~\cite{stiennon2020learning, ouyang2022training}.
	\item \textbf{Cycle / Transitivity Checks:} Identify preference cycles (A$\succ$B, B$\succ$C, C$\succ$A) as indicators of either ambiguity or annotation noise; down-weight those edges when estimating latent utilities~\cite{bradley1952rank, liu2024pairs}.
	\item \textbf{Fairness-Disagreement Audits:} Examine whether disagreement concentrates in particular demographic or linguistic slices to avoid conflating cultural variance with judge bias~\cite{hada2024metal, liang2022helm}.
	\item \textbf{Soft-Target Training:} Train reward / judge models directly on empirical preference probabilities (e.g., 0.6 vs 0.4) rather than binarized labels to retain uncertainty and potentially improve calibration~\cite{stiennon2020learning, ouyang2022training}.
\end{itemize}

\subsection{Reliability and Generalization of Fine-Tuned / RLHF Judges}
Reward / judge models are one promising approach to approximate latent human utility but may: (i) overweight superficial features (verbosity, specific lexical tokens), (ii) exhibit prompt and order sensitivity, (iii) lose calibration when distribution shifts, (iv) carry forward training-set biases across languages or domains, and (v) become stale as frontier model behaviors evolve~\cite{li2024_llmsasjudges, bavaresco2024judgebench, hada2024metal}. These factors can induce ranking instability, fairness disparities, or inconsistent pairwise decisions unseen during initial validation.

Luckily, we see emerging best practices and research directions to also address these issues. Among others, these include:
\begin{itemize}
	\item \textbf{Diverse, Continual Benchmarking:} Periodically re-evaluate judges across heterogeneous tasks, languages, and prompt templates (HELM-style scenario coverage) to detect drift~\cite{liang2022helm, hada2024metal}.
	\item \textbf{Ensemble / Prompt Diversification:} Aggregate multiple judge prompts or model variants to reduce single-prompt bias and variance; flag high-variance cases for human review~\cite{li2024_llmsasjudges, zheng2023judgelm}.
	\item \textbf{Robustness Stress Testing:} Systematically perturb order, formatting, and paraphrases to quantify instability and gate deployment~\cite{zheng2023judgelm, li2024_llmsasjudges}.
	\item \textbf{Hybrid Objective + Subjective Signals:} Combine execution tests (code), factuality probes, or retrieval-grounding checks with judge scores to anchor subjective assessments~\cite{chen2021evaluating, liang2022helm}.
\end{itemize}

\subsection{Interdependence of the Two Challenges}
Disagreement-aware evaluation and robust learned judges are mutually reinforcing. Without modeling human uncertainty, we risk mis-attributing irreducible ambiguity to judge error; without stable, generalizable judges, we falsely interpret model-induced variance as label noise. Future work likely converges on joint frameworks that (i) treat human preference as a probabilistic object with explicit uncertainty, and (ii) train or select judges under distributionally diverse, robustness- and calibration-aware criteria.~\cite{li2024_llmsasjudges, liang2022helm, bavaresco2024judgebench}.