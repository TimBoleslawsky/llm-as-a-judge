
\section{Limitations and Challenges of LLM-as-Judge Evaluators}
This section highlights two foundational, cross-cutting open challenges that underlie many surface-level issues (robustness, fairness, calibration, transfer) discussed elsewhere.

\subsection{Challenge 1: Human Annotator Disagreement and Uncertain Preferences}
\paragraph{Why it Matters.} Human comparison or scoring data supply the reference signal for virtually all agreement, correlation, and calibration metrics used to validate judges~\cite{stiennon2020learning, ouyang2022training, bavaresco2024judgebench}. Intrinsic variability among raters (task ambiguity, subjective stylistic preferences, cultural and linguistic differences) creates a ceiling on attainable judge--human agreement~\cite{bavaresco2024judgebench, li2024_llmsasjudges}. Treating noisy, heterogeneous judgments as if they were deterministic ground truth risks (i) over-penalizing judges on ambiguous items, and (ii) masking progress when improvements lie within the noise envelope.
\paragraph{What the Challenge Is.} Rather than a single latent ordering, many tasks yield a \emph{distribution} over plausible preferences for response pairs or rankings. Pairwise signals are frequently modeled via probabilistic frameworks (e.g., Bradley--Terry, Elo) acknowledging stochastic choice~\cite{bradley1952rank, elo1978rating, liu2024pairs}. However, meta-evaluation pipelines often collapse multiple votes into a single label without carrying forward uncertainty (vote entropy, variance). This conflation distorts rank correlation interpretation, inflates apparent calibration error on ambiguous pairs, and can bias fairness audits if disagreement clusters in specific subgroups or languages~\cite{hada2024metal, liang2022helm}.
\paragraph{Mitigation Directions.} The literature and emerging practice suggest several strategies:
\begin{itemize}
	\item \textbf{Collect Multiple Votes and Model Stochastically:} Use overlapping pairwise comparisons and fit Bradley--Terry/Elo style models to infer latent strengths and confidence intervals~\cite{bradley1952rank, elo1978rating, stiennon2020learning, ouyang2022training, liu2024pairs}.
	\item \textbf{Report Agreement Ceilings:} Publish inter-annotator agreement or vote entropy distributions so judge metrics can be normalized to an empirical upper bound~\cite{bavaresco2024judgebench, li2024_llmsasjudges}.
	\item \textbf{Consensus-Aware Weighting:} Weight comparisons by consensus strength (e.g., inverse entropy) or stratify metrics by consensus tiers to distinguish hard ambiguity from systematic judge error~\cite{bavaresco2024judgebench}.
	\item \textbf{Ambiguity-Aware Calibration:} Compute calibration / ECE separately on high-consensus subsets and probabilistic (soft label) versions that use empirical win rates as targets~\cite{guo2017calibration, brier1950verification}.
	\item \textbf{Active / Targeted Adjudication:} Escalate only high-disagreement or high-impact pairs to expert adjudicators to refine a trusted ``gold'' subset~\cite{stiennon2020learning, ouyang2022training}.
	\item \textbf{Cycle / Transitivity Checks:} Identify preference cycles (A$\succ$B, B$\succ$C, C$\succ$A) as indicators of either ambiguity or annotation noise; down-weight those edges when estimating latent utilities~\cite{bradley1952rank, liu2024pairs}.
	\item \textbf{Fairness-Disagreement Audits:} Examine whether disagreement concentrates in particular demographic or linguistic slices to avoid conflating cultural variance with judge bias~\cite{hada2024metal, liang2022helm}.
	\item \textbf{Soft-Target Training:} Train reward / judge models directly on empirical preference probabilities (e.g., 0.6 vs 0.4) rather than binarized labels to retain uncertainty and potentially improve calibration~\cite{stiennon2020learning, ouyang2022training}.
\end{itemize}

\subsection{Challenge 2: Reliability and Generalization of Fine-Tuned / RLHF Judges}
\paragraph{Why it Matters.} Learned judges (reward models or fine-tuned LLM evaluators) promise scalability but can overfit collection-specific artifacts (style biases, prompt conventions) and degrade under domain, task, or language shift~\cite{stiennon2020learning, ouyang2022training, hada2024metal, kocmi2023llmmt}. Empirical audits show variability across tasks and sensitivity to prompt/order formatting~\cite{bavaresco2024judgebench, zheng2023judgelm, li2024_llmsasjudges}. Mis-generalization risks mis-ranking models, undermining leaderboards, and propagating bias or instability into automated evaluation pipelines.~\cite{liang2022helm} further argues for multi-metric contextualization to avoid over-trusting a single scalar learned score.
\paragraph{What the Challenge Is.} Reward / judge models approximate latent human utility but may: (i) overweight superficial features (verbosity, specific lexical tokens), (ii) exhibit prompt and order sensitivity, (iii) lose calibration when distribution shifts, (iv) carry forward training-set biases across languages or domains, and (v) become stale as frontier model behaviors evolve~\cite{li2024_llmsasjudges, bavaresco2024judgebench, hada2024metal}. These factors can induce ranking instability, fairness disparities, or inconsistent pairwise decisions unseen during initial validation.
\paragraph{Mitigation Directions.}
\begin{itemize}
	\item \textbf{Diverse, Continual Benchmarking:} Periodically re-evaluate judges across heterogeneous tasks, languages, and prompt templates (HELM-style scenario coverage) to detect drift~\cite{liang2022helm, hada2024metal}.
	\item \textbf{Ensemble / Prompt Diversification:} Aggregate multiple judge prompts or model variants to reduce single-prompt bias and variance; flag high-variance cases for human review~\cite{li2024_llmsasjudges, zheng2023judgelm}.
	\item \textbf{Robustness Stress Testing:} Systematically perturb order, formatting, and paraphrases to quantify instability and gate deployment~\cite{zheng2023judgelm, li2024_llmsasjudges}.
	\item \textbf{Hybrid Objective + Subjective Signals:} Combine execution tests (code), factuality probes, or retrieval-grounding checks with judge scores to anchor subjective assessments~\cite{chen2021evaluating, liang2022helm}.
	\item \textbf{Uncertainty-Aware Escalation:} Use calibrated confidence (or ensemble variance) to auto-accept only high-confidence judgments and route uncertain cases to humans~\cite{guo2017calibration, brier1950verification}.
	\item \textbf{Regularization and Soft Targets:} Employ soft preference probabilities and penalize over-confident predictions to improve calibration under shift~\cite{stiennon2020learning, ouyang2022training, niculescu2005predicting}.
	\item \textbf{Cross-Lingual / Cross-Domain Adaptation:} Use multilingual meta-evaluation (e.g., METAL) and targeted adaptation for low-resource or domain-shifted scenarios~\cite{hada2024metal, kocmi2023llmmt}.
	\item \textbf{Audit Latent Dimension Coverage:} Map which quality dimensions (factuality, coherence, safety, style) the judge implicitly weights via diagnostic prompts or ablations to detect blind spots~\cite{li2024_llmsasjudges, liang2022helm}.
\end{itemize}

\subsection{Interdependence of the Two Challenges}
Disagreement-aware evaluation and robust learned judges are mutually reinforcing. Without modeling human uncertainty, we risk mis-attributing irreducible ambiguity to judge error; without stable, generalizable judges, we falsely interpret model-induced variance as label noise. Future work likely converges on joint frameworks that (i) treat human preference as a probabilistic object with explicit uncertainty, and (ii) train or select judges under distributionally diverse, robustness- and calibration-aware criteria.~\cite{li2024_llmsasjudges, liang2022helm, bavaresco2024judgebench}.