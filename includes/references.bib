@inproceedings{papineni2002bleu,
  title={BLEU: a Method for Automatic Evaluation of Machine Translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  address={Philadelphia, PA, USA},
  doi={10.3115/1073083.1073135}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Lin, Chin-Yew},
  booktitle={Proceedings of the ACL Workshop on Text Summarization Branches Out},
  pages={74--81},
  year={2004},
  address={Barcelona, Spain},
  url={https://aclanthology.org/W04-1013/}
}

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William H. and Nichol, Alex and Paino, Igor and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  note={HumanEval dataset introduced; DOI: 10.48550/arXiv.2107.03374},
  url={https://arxiv.org/abs/2107.03374}
}

@article{fu2023gptscore,
  title={GPTScore: Evaluate as You Desire},
  author={Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  journal={arXiv preprint arXiv:2302.04166},
  year={2023},
  url={https://arxiv.org/abs/2302.04166}
}

@article{zheng2023judgelm,
  title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph and Stoica, Ion},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023},
  url={https://arxiv.org/abs/2306.05685}
}

@article{li2024_llmsasjudges,
  title={LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods},
  author={Li, Haitao and Dong, Qian and Chen, Junjie and Su, Huixue and Zhou, Yujia and Ai, Qingyao and Ye, Ziyi and Liu, Yiqun},
  journal={arXiv preprint arXiv:2412.05579},
  year={2024},
  url={https://arxiv.org/abs/2412.05579}
}

@misc{alpacaeval2023,
  title={AlpacaEval: An Automatic Evaluator for Instruction-following Models},
  author={Dubois, Yann and Li, Weixin and Huang, Yizhong and Taori, Rohan and Zhang, Tianyi and Liang, Percy and the Tatsu Lab contributors},
  year={2023},
  howpublished={\url{https://github.com/tatsu-lab/alpaca_eval}},
  note={GitHub repository (AlpacaEval)}
}

@article{liu2024pairs,
  title={Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators},
  author={Liu, Yinhong and others},
  journal={arXiv preprint arXiv:2403.16950},
  year={2024},
  url={https://arxiv.org/abs/2403.16950}
}

@article{hada2024metal,
  title={METAL: Towards Multilingual Meta-Evaluation},
  author={Hada, Rishav and Gumma, Varun and Ahmed, Mohamed and Bali, Kalika and Sitaram, Sunayana},
  journal={arXiv preprint arXiv:2404.01667},
  year={2024},
  url={https://arxiv.org/abs/2404.01667}
}

@article{bavaresco2024judgebench,
  title={LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks (JudgeBench)},
  author={Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fern\'andez, Raquel and Gatt, Albert and Giulianelli, Mario and others},
  journal={arXiv preprint arXiv:2406.18403},
  year={2024},
  url={https://arxiv.org/abs/2406.18403}
}

@article{zhang2023widedeep,
  title={Wider and Deeper LLM Networks are Fairer LLM Evaluators},
  author={Zhang, Xinghua and Yu, Bowen and Yu, Haiyang and Lv, Yangyu and Liu, Tingwen and Huang, Fei and Xu, Hongbo and Li, Yongbin},
  journal={arXiv preprint arXiv:2308.01862},
  year={2023},
  url={https://arxiv.org/abs/2308.01862}
}

@misc{microsoftmetrics2024,
  title={List of LLM Evaluation Metrics},
  author={{Microsoft}},
  year={2024},
  howpublished={\url{https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/list-of-eval-metrics}},
  note={Microsoft AI Playbook — evaluation metrics}
}

@misc{analyticsvidhya2025,
  title={LLM Evaluation Metrics: Everything You Need to Know},
  author={{Analytics Vidhya}},
  year={2025},
  howpublished={\url{https://www.analyticsvidhya.com/blog/2025/03/llm-evaluation-metrics}},
  note={Blog / tutorial}
}

@misc{confidentai2024,
  title={LLM Evaluation Metrics: Everything You Need},
  author={{Confident AI}},
  year={2024},
  howpublished={\url{https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation}},
  note={Blog / industry article}
}

@misc{evidently2024,
  title={LLM as a Judge Guide},
  author={{Evidently AI}},
  year={2024},
  howpublished={\url{https://www.evidentlyai.com/llm-guide/llm-as-a-judge}},
  note={Industry guide / blog}
}

@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Zhao, Collin and Basart, Szymon and D’Amour, Alexandre and Desai, Akshat and Jiang, Eric and Laskin, Mark and Song, Lily and Wang, Xue and others},
  journal={arXiv preprint arXiv:2110.02682},
  year={2021},
  url={https://arxiv.org/abs/2110.02682},
  note={(MMLU / multi-task evaluation benchmark)}
}

@article{liang2022helm,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bombare, Rishi and Chen, Roy and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022},
  url={https://arxiv.org/abs/2211.09110},
  note={HELM framework defining multi-metric, scenario-based LM evaluation}
}

@article{srivastava2022bigbench,
  title={Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
  author={Srivastava, A and Rastogi, A and Rao, A and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022},
  url={https://arxiv.org/abs/2206.04615},
  note={BIG-bench; large collaborative benchmark introducing capability probing tasks}
}

@article{kocmi2023llmmt,
  title={Large Language Models are State-of-the-Art for Zero-Shot Machine Translation Evaluation},
  author={Kocmi, Tom and Federmann, Christian},
  journal={arXiv preprint arXiv:2302.14520},
  year={2023},
  url={https://arxiv.org/abs/2302.14520},
  note={Shows LLM-as-judge effectiveness for MT evaluation tasks}
}

@inproceedings{stiennon2020learning,
  title={Learning to Summarize with Human Feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M and Lowe, Ryan J and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
  url={https://arxiv.org/abs/2009.01325},
  note={Establishes human preference modeling and RLHF evaluation loop}
}

@article{ouyang2022training,
  title={Training Language Models to Follow Instructions with Human Feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
  url={https://arxiv.org/abs/2203.02155},
  note={Defines instruction-following evaluation via human preference datasets}
}

@article{bradley1952rank,
  title={Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  doi={10.2307/2334029},
  note={Bradley--Terry model underpinning probabilistic pairwise preference aggregation}
}

@book{elo1978rating,
  title={The Rating of Chessplayers, Past and Present},
  author={Elo, Arpad E},
  publisher={Arco Publishing},
  year={1978},
  note={Elo rating system; adapted for iterative LLM response ranking (e.g., Chatbot Arena)}
}

@inproceedings{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  year={2017},
  url={https://arxiv.org/abs/1706.04599},
  note={Introduces reliability diagrams and Expected Calibration Error (ECE)}
}

@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W},
  journal={Monthly Weather Review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950},
  doi={10.1175/1520-0493(1950)078<0001:VOFETI>2.0.CO;2},
  note={Defines the Brier score for probabilistic predictions}
}

@inproceedings{niculescu2005predicting,
  title={Predicting Good Probabilities with Supervised Learning},
  author={Niculescu-Mizil, Alexandru and Caruana, Rich},
  booktitle={Proceedings of the 22nd International Conference on Machine Learning},
  year={2005},
  pages={625--632},
  doi={10.1145/1102351.1102430},
  note={Compares probability calibration methods; motivates post-hoc calibration}
}