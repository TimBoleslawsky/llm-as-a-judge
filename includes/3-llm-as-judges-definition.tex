\section{LLMs as a Judge: A Definition}

\subsection{Why Use LLMs as Judges}
LLMs are adopted as evaluators because they can approximate human quality and preference judgments across diverse tasks at far lower marginal cost and latency than repeated human annotation, while achieving strong agreement and stable comparative rankings in structured protocols~\cite{li2024_llmsasjudges, zheng2023judgelm, bavaresco2024judgebench}. Traditional surface-form overlap metrics (e.g., BLEU, ROUGE) miss semantic fidelity and instruction adherence~\cite{papineni2002bleu, lin2004rouge}, whereas prompt- or preference-driven LLM judges can assess nuanced dimensions like reasoning depth, contextual grounding, and alignment~\cite{fu2023gptscore, stiennon2020learning, ouyang2022training}.

\subsection{Core Components of an LLM-as-a-Judge Evaluator}
We distill essential components (survey foundation in~\cite{li2024_llmsasjudges}) and supporting empirical frameworks. Further detail on the essential components will be discussed in future sections.
\begin{enumerate}
	\item \textbf{Evaluation Scope / Scenarios}: Definition of task and domain coverage.~\cite{liang2022helm, srivastava2022bigbench}. This includes failure mode ($X$ \& $C$, $Y$, or mapping from $X$ \& $C$ to $Y$).
	\item \textbf{Evaluation Paradigm}: Pointwise scoring, pairwise preference, pass/fail (objective tests), or tournament / listwise aggregation; see section 2 B.
	\item \textbf{Judge Model Type}: Prompt-engineered base LLM, fine-tuned reward / preference model, or ensemble for variance and bias reduction~\cite{zhu2023judgelm, bavaresco2024judgebench}.
	\item \textbf{Prompt Template}: Dimension specification (accuracy, faithfulness, style, safety), critique-first prompting, and structured output schema~\cite{fu2023gptscore, li2024_llmsasjudges}.
	\item \textbf{Meta-Evaluation Metrics}: Agreement (pairwise accuracy, rank correlation), calibration, robustness (order / paraphrase), fairness (subgroup disparity), consistency (variance), and efficiency / cost~\cite{bavaresco2024judgebench, hada2024metal, li2024_llmsasjudges}.
	\item \textbf{Objective Signals (When Available)}: Execution-based or other task-grounded checks (e.g., unit tests for code) complement subjective judgment~\cite{chen2021evaluating, li2024_llmsasjudges}.
\end{enumerate} 

These components collectively compose the evaluator and are necessary to construct a systematic, reliable, and interpretable LLM-as-a-judge evaluator.