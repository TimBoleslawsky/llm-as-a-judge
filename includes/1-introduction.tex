\section{Introduction}
Large Language Models (LLMs) have become increasingly central to automated code generation and evaluation tasks. Their ability to both produce solutions and assess responses has opened new research avenues, but also raised questions about reliability, bias, and alignment with human judgment. A crucial research strand focuses on \textit{LLMs as a judge}. The evaluation of Large Language Model (LLM) outputs increasingly relies on \emph{LLMs-as-Judges} to approximate human quality judgments across diverse tasks~\cite{li2024_llmsasjudges, zheng2023judgelm, bavaresco2024judgebench}. Within this literature review we want to provide a comprehensive overview of the current state of research on LLMs as judges, including definitions, methodologies, applications, and open challenges. 