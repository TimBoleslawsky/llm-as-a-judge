\section{Applications and Problem Settings for LLM-as-Judge} %TBD
We summarize representative use cases and problem settings where LLM-based evaluators are applied. For meta-evaluation criteria and judge reliability, see Chapter~5.

\subsection{Instruction Following and Dialogue Helpfulness}
Judges assess adherence to user instructions, helpfulness/harmlessness, and style constraints, typically via pairwise comparisons or rubric-based scoring~\cite{zheng2023judgelm, li2024_llmsasjudges}. Common pitfalls include verbosity bias and order sensitivity.

\subsection{Code Generation}
Execution-based pass/fail (e.g., HumanEval) provides objective correctness, with judges complementing tests on style, clarity, and alignment with requirements~\cite{chen2021evaluating, li2024_llmsasjudges}.

\subsection{Summarization and Machine Translation}
LLM judges capture semantic adequacy and faithfulness beyond surface overlap metrics; pairwise protocols and scenario stratification improve reliability~\cite{fu2023gptscore, liang2022helm}. Domain and language shifts remain important considerations~\cite{hada2024metal}.

\subsection{Retrieval-Augmented and Grounded Tasks}
Evaluators check consistency between responses and provided context (evidence grounding), in addition to instruction adherence. Multi-axis reporting helps isolate context quality from response quality~\cite{liang2022helm, li2024_llmsasjudges}.

\subsection{Leaderboards and Model Selection}
Arena-style pairwise frameworks aggregate outcomes to rank models, informing deployment choices and ablations~\cite{zheng2023judgelm}. Confidence intervals and slice analysis help interpret close rankings~\cite{bavaresco2024judgebench}.

\subsection{Multilingual and Multimodal Settings}
Multilingual evaluations expose disparities in judge reliability and require language-aware protocols and balancing~\cite{hada2024metal}. (Multimodal is analogous but beyond our current scope.)

\subsection{Safety and Moderation}
Judges are used to evaluate safety policies and detect policy violations as part of broader moderation pipelines~\cite{li2024_llmsasjudges}. Robustness to adversarial prompts remains a challenge.

\subsection{Operational Notes}
Across applications, reporting by scenario/language slices and tracking robustness, calibration, and fairness remain essential for trustworthy use (see Chapter~5)~\cite{liang2022helm, bavaresco2024judgebench}.