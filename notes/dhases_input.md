Kind of artefacts we want to evaluate: 
- Text (30%)
- Code (70%)

Data sets to use for this artefacts:
- Public benchmark 
- Volvo synthetic data to be created (50ish) => we can’t do this with python?? 

Thinking about the LLM workflow as Req -> LLM -> Resp, we want to answer:
- Is Resp evaluator “solved”?
- Req to Resp evaluator (What are the components?, How do we calibrate it?)
- Req evaluator (-“-)